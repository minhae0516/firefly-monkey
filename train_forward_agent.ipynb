{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is for training agent by using DDPG.\n",
    "\"\"\"\n",
    "Three time variables\n",
    "tot_t: number of times step since the code started\n",
    "episode: the number of fireflies since the code started\n",
    "t: number of time steps for the current firefly\n",
    "\"\"\"\n",
    "from DDPGv2Agent import Agent\n",
    "from DDPGv2Agent.noise import *\n",
    "from FireflyEnv import Model # firefly_task.py\n",
    "from collections import deque\n",
    "from DDPGv2Agent.rewards import return_reward\n",
    "from Config import Config\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# read configuration parameters\n",
    "arg = Config()\n",
    "\n",
    "\n",
    "# fix random seed\n",
    "import random\n",
    "random.seed(arg.SEED_NUMBER)\n",
    "import torch\n",
    "torch.manual_seed(arg.SEED_NUMBER)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(arg.SEED_NUMBER)\n",
    "import numpy as np\n",
    "np.random.seed(arg.SEED_NUMBER)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "FloatTensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if CUDA else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "rewards = deque(maxlen=500)\n",
    "hit_ratio_log = deque(maxlen=500)\n",
    "time_log = deque(maxlen=500)\n",
    "value_losses = deque(maxlen=500)\n",
    "batch_size = arg.BATCH_SIZE\n",
    "hit_log = []\n",
    "avg_hit_ratio =[]\n",
    "value_loss_log = []\n",
    "policy_loss_log = []\n",
    "rewards_log = []\n",
    "rpt_tot = deque(maxlen=50) # reward per time (for fixed duration)\n",
    "rpt_tot.append(0)\n",
    "policy_loss, value_loss = torch.zeros(1), torch.zeros(1) # initialization\n",
    "finetuning = 0 # this is the flag to indicate whether finetuning mode or not (if it is finetuning mode: reward is based on real location)\n",
    "AGENT_STORE_FRQ = 1 #25\n",
    "\n",
    "\n",
    "COLUMNS = ['total time', 'ep', 'std', 'time step', 'Policy NW loss', 'value NW loss','reward','avg_reward', 'goal',\n",
    "           'a_vel', 'a_ang', 'true_r',\n",
    "           'r', 'rel_ang', 'vel', 'ang_vel',\n",
    "           'vecL1','vecL2','vecL3','vecL4','vecL5','vecL6','vecL7','vecL8','vecL9','vecL10',\n",
    "           'vecL11','vecL12','vecL13','vecL14','vecL15',\n",
    "           'process gain forward', 'process gain angular', 'process noise std forward', 'process noise std angular',\n",
    "           'obs gain forward', 'obs gain angular', 'obs noise std forward', 'obs noise std angular', 'goal radius',\n",
    "           'batch size', 'box_size', 'std_step_size', 'discount_factor', 'num_epochs']\n",
    "\n",
    "ep_time_log = pd.DataFrame(columns=COLUMNS)\n",
    "\n",
    "env = Model(arg) # build an environment\n",
    "x, pro_gains, pro_noise_stds, goal_radius = env.reset(arg.gains_range, arg.std_range, arg.goal_radius_range)\n",
    "\n",
    "\n",
    "\n",
    "tot_t = 0. # number of total time steps\n",
    "episode = 0. # number of fireflies\n",
    "int_t = 1 # variable for changing the world setting every EPISODE_LEN time steps\n",
    "\n",
    "state_dim = env.state_dim\n",
    "action_dim = env.action_dim\n",
    "filename = arg.filename\n",
    "\n",
    "argument = {'filename': filename,\n",
    "            'argument': arg.__dict__}\n",
    "torch.save(argument, '../firefly-monkey-data/data/'+filename+'_arg.pkl')\n",
    "\n",
    "agent = Agent(state_dim, action_dim, arg,  filename, hidden_dim=128, gamma=arg.DISCOUNT_FACTOR, tau=0.001)\n",
    "\n",
    "#\"\"\"\n",
    "# if you want to use pretrained agent, load the data as below\n",
    "# if not, comment it out\n",
    "#agent.load('20191004-160540')\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "b, state, obs_gains, obs_noise_stds = agent.Bstep.reset(x, torch.zeros(1), pro_gains, pro_noise_stds, goal_radius, arg.gains_range, arg.std_range)  # reset monkey's internal model\n",
    "\n",
    "\n",
    "# action space noise\n",
    "std = 0.4 # this is for action space noise for exploration\n",
    "noise = Noise(action_dim, mean=0., std=std)\n",
    "\n",
    "\n",
    "while tot_t <= arg.TOT_T:\n",
    "    episode += 1 # every episode starts a new firefly\n",
    "    t = torch.zeros(1) # to track the amount of time steps to catch a firefly\n",
    "\n",
    "    theta = (pro_gains, pro_noise_stds, obs_gains, obs_noise_stds, goal_radius)\n",
    "\n",
    "    while t < arg.EPISODE_LEN: # for a single FF\n",
    "        action = agent.select_action(state, action_noise = noise, param = None)  # with action noise\n",
    "\n",
    "\n",
    "        next_x, reached_target = env(x, action.view(-1)) #track true next_x of monkey\n",
    "        next_ox = agent.Bstep.observations(next_x)  # observation\n",
    "        next_b, info = agent.Bstep(b, next_ox, action, env.box) # belief next state, info['stop']=terminal # reward only depends on belief\n",
    "        next_state = agent.Bstep.Breshape(next_b, t, theta) # state used in policy is different from belief\n",
    "\n",
    "        # reward\n",
    "        reward = return_reward(episode, info, reached_target, next_b, env.goal_radius, arg.REWARD, finetuning)\n",
    "\n",
    "        # check time limit\n",
    "        TimeEnd = (t+1 == arg.EPISODE_LEN) # if the monkey can't catch the firefly in EPISODE_LEN, reset the game.\n",
    "        mask = torch.tensor([1 - float(TimeEnd)]) # mask = 0: episode is over\n",
    "\n",
    "        data = np.array([[tot_t, episode, std, t, policy_loss.item(), value_loss.item(), reward, np.mean(rewards),\n",
    "                          reached_target.item(), action[0][0].item(), action[0][1].item(), torch.norm(x.view(-1)[0:2]).item(),\n",
    "                          state[0][0].item(), state[0][1].item(), state[0][2].item(), state[0][3].item(),\n",
    "                          state[0][5].item(), state[0][6].item(), state[0][7].item(), state[0][8].item(),\n",
    "                          state[0][9].item(),\n",
    "                          state[0][10].item(), state[0][11].item(), state[0][12].item(), state[0][13].item(),\n",
    "                          state[0][14].item(),\n",
    "                          state[0][15].item(), state[0][16].item(), state[0][17].item(), state[0][18].item(),\n",
    "                          state[0][19].item(),\n",
    "                          pro_gains[0].item(), pro_gains[1].item(), pro_noise_stds[0].item(), pro_noise_stds[1].item(),\n",
    "                          obs_gains[0].item(), obs_gains[1].item(), obs_noise_stds[0].item(), obs_noise_stds[1].item(),\n",
    "                          goal_radius.item(),\n",
    "                          arg.BATCH_SIZE, arg.WORLD_SIZE, arg.STD_STEP_SIZE, arg.DISCOUNT_FACTOR, arg.NUM_EPOCHS]])\n",
    "\n",
    "        df1 = pd.DataFrame(data, columns=COLUMNS)\n",
    "        ep_time_log = ep_time_log.append(df1)\n",
    "\n",
    "        if info['stop'] or TimeEnd:  # if the monkey stops or pass the time limit, start the new firefly\n",
    "            next_x, pro_gains, pro_noise_stds, goal_radius = env.reset(arg.gains_range, arg.std_range, arg.goal_radius_range)\n",
    "            next_b, next_state, obs_gains, obs_noise_stds = agent.Bstep.reset(next_x, torch.zeros(1), pro_gains, pro_noise_stds, goal_radius, arg.gains_range, arg.std_range)  # reset monkey's internal model\n",
    "\n",
    "        agent.memory.push(state, action, 1 - mask, next_state, reward)\n",
    "\n",
    "\n",
    "\n",
    "        if len(agent.memory) > 1000:\n",
    "            policy_loss, value_loss = agent.learn(batch_size=batch_size)\n",
    "            policy_loss_log.append(policy_loss.data.clone().item())\n",
    "            value_losses.append(value_loss.data.clone().item())\n",
    "            if len(agent.memory) > 2000 and tot_t % 500 == 0:\n",
    "                value_loss_log.append(np.mean(value_losses))\n",
    "\n",
    "\n",
    "\n",
    "        if tot_t % 2500 == 0 and tot_t != 0:\n",
    "            agent.save(filename, episode)\n",
    "\n",
    "        # update variables\n",
    "        x = next_x\n",
    "        state = next_state\n",
    "        b = next_b\n",
    "        t += 1.\n",
    "        tot_t += 1.\n",
    "        rewards.append(reward[0].item())\n",
    "\n",
    "        if tot_t % 100 == 0:\n",
    "            # update action space exploration noise\n",
    "            std -= arg.STD_STEP_SIZE  # exploration noise\n",
    "            std = max(0.05, std)\n",
    "            noise.reset(0., std)\n",
    "\n",
    "        if tot_t % 500 == 0 and tot_t != 0:\n",
    "            rewards_log.append(np.mean(rewards))\n",
    "\n",
    "\n",
    "        if info['stop'] or TimeEnd: # if the monkey stops or pass the time limit, start the new firefly\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    hit = (info['stop']) * reached_target\n",
    "    hit_log.append(hit)\n",
    "    hit_ratio_log.append(hit)\n",
    "    avg_hit_ratio.append(np.mean(hit_ratio_log))\n",
    "    time_log.append(t) # time for recent 50 episodes\n",
    "    policy_loss_log.append(policy_loss.item())\n",
    "    value_loss_log.append(value_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if episode % 500 == 0 and episode != 0:\n",
    "        ep_time_log.to_csv(path_or_buf='../firefly-monkey-data/data/' + filename + '_log.csv', index=False)\n",
    "\n",
    "        print(\"Ep: {}, steps: {}, std: {:0.2f}, box:{:0.2f}, min_goal_radius: {:0.2f}, ave_reward: {:0.3f}, hit ratio: {:0.3f}\".format(episode, np.mean(time_log), noise.scale, env.box, env.min_goal_radius, np.mean(rewards), avg_hit_ratio[-1]))\n",
    "        plt.figure()\n",
    "        plt.plot(policy_loss_log)\n",
    "        plt.savefig('../firefly-monkey-data/data/' + filename + 'policy_loss_log' + '.jpg', format='jpg')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(value_loss_log)\n",
    "        plt.savefig('../firefly-monkey-data/data/' + filename + 'value_loss_log' + '.jpg', format='jpg')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(rewards_log)\n",
    "        plt.savefig('../firefly-monkey-data/data/' + filename + 'av_reward_log' + '.jpg', format='jpg')\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "        if CUDA:\n",
    "            print(torch.cuda.get_device_name(0))\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024, 1), 'kB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_cached(0) / 1024, 1), 'kB')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
